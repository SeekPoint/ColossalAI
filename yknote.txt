https://github.com/SeekPoint/ColossalAI

amd00@MZ32-00:~/yk_repo/ColossalAI$
sudo docker pull hpcaitech/colossalai:0.3.0



d00@MZ32-00:~/yk_repo/ColossalAI$ sudo docker build -t colossalai ./docker


amd00@MZ32-00:~/yk_repo/ColossalAI$ sudo docker images
REPOSITORY                    TAG                            IMAGE ID       CREATED          SIZE
colossalai                    latest                         1f2f6a3e1dcf   15 minutes ago   17.2GB

# 浅析Docker容器的两种运行模式及 docker run 的 --rm 参数的作用及与 docker rm 的区别
sudo docker run --gpus=all --rm -it -v /home/amd00:/workspace -v /data:/share -w /workspace/yk_repo/ColossalAI --ipc=host --name colossal --shm-size="100G" colossalai:latest  #1f2f6a3e1dcf
sudo docker run --gpus=all -it -v /home/amd00:/workspace -v /data:/share -w /workspace/yk_repo/ColossalAI --ipc=host --name colossal --shm-size="100G" colossalai:latest  #1f2f6a3e1dcf

在Docker容器退出时，默认容器内部的文件系统仍然被保留，加上--rm就会被删除

所有pip可以用加速
-i https://pypi.tuna.tsinghua.edu.cn/simple
         pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
Collecting transformers>=4.23

====================================================================================================
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/opt# bash run_demo.sh
错误
ImportError: /opt/conda/lib/python3.9/site-packages/_cffi_backend.cpython-39-x86_64-linux-gnu.so: undefined symbol: ffi_type_uint32, version LIBFFI_BASE_7.0
解决
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/opt# pip install --upgrade cffi==1.14.0
Collecting cffi==1.14.0
目前卡在
RuntimeError: FlashAttention only supports Ampere GPUs or newer.

===================================================================================================

root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/gpt/hybridparallelism# bash run.sh
---修改若干参数后可以到达
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(
RuntimeError: FlashAttention only supports Ampere GPUs or newer.
----只能暂时强制屏蔽FlashAttention！


====================================================================================================

root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/gpt/gemini# bash run_gemini.sh
+ export DISTPLAN=CAI_Gemini
+ DISTPLAN=CAI_Gemini
+ export GPUNUM=1
====跑通！@
===================================================================================================
warnings.warn("please install xformers from https://github.com/facebookresearch/xformers")
必须修正
root@83dccafe7db4:/workspace/yk_repo/ColossalAI#
pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
或者源码安装
git clone https://github.com/facebookresearch/xformers.git
cd xformers
git submodule update --init --recursive
pip install -r requirements.txt
pip install -e .

===================================================================================================
错误
    # https://stackoverflow.com/questions/68084302/assertionerror-cannot-handle-batch-sizes-1-if-no-padding-token-is-defined
    # "AssertionError: Cannot handle batch sizes > 1 if no padding token is > defined" and pad_token = eos_token
修正    cfg.pad_token_id = cfg.eos_token_id

========================

module 'torch' has no attribute 'fx' 错误，
解决
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/llama2#
pip install transformers==4.34.0 -i https://pypi.tuna.tsinghua.edu.cn/simple

more tests:
4.35.0-4.35.2: same error
4.34.0-4.34.1: succeed

===================
跑通
mkdir -p /logColossalAI/_log_tmps_llama2_/

root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/llama2# colossalai run --nproc_per_node 3 pretrain.py
Epoch 0:   2%|▏         | 45/2423
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/llama2# colossalai run --nproc_per_node 2 pretrain.py
Epoch 0:  12%|█▏        | 445/3634
可以看出，同样bs下，增加显卡数量可以减少迭代次数，

# 在两个节点上训练

--master_port=29502  ===失败后重试需要修改！因为已经打开被占用

colossalai run --nproc_per_node 2 --host 192.168.1.11,192.168.1.12 --master_port=29507 --master_addr 192.168.1.11 pretrain.py
colossalai run --nproc_per_node 2 --hostfile ./hostfile --master_addr 192.168.1.11 pretrain.py
--hostfile ./hostfile

清理log rm -rf f /logColossalAI/_log_tmps_llama2_/



===transformers==4.34.0

查看nccl版本
python3 -c "import torch;print(torch.cuda.nccl.version())"

colossalai多机多卡训练，机器间通讯不了，或服务
https://github.com/hpcaitech/ColossalAI/issues/2958

export NCCL_DEBUG=INFO
export NCCL_SOCKET_IFNAME='en0'


=========================可行的例子！！！！！！！！！
https://zhuanlan.zhihu.com/p/636605130
算法框架-LLM-4-ColossalAI源码分析(ing)
=========================================
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/applications/Chat# pip install -e .

root@83dccafe7db4:/workspace/yk_repo/ColossalAI/applications/Chat#
torchrun --standalone --nproc_per_node=3 /workspace/yk_repo/ColossalAI/applications/Chat/examples/train_sft.py \
--pretrain "/share/hf_model/bloomz-560m/" --model 'bloom' --strategy colossalai_zero2 --save_path Coati-bloom-560m-sft \
--dataset /share/hf_model/instinwild_ch.json --batch_size 8 --accumulation_steps 8 --lr 2e-5 --max_datasets_size 512 --max_epochs 3
passed
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/applications/Chat#
torchrun --standalone --nproc_per_node=3 /workspace/yk_repo/ColossalAI/applications/Chat/examples/train_sft.py --pretrain "/share/hf_model/llama-7b-hf" --model 'llama' --strategy colossalai_zero2_cpu --save_path /workspace/ColossalAI/Saved/Coati-llama-7b-sft --dataset /share/hf_model/instinwild_ch.json --batch_size 1 --accumulation_steps 2 --lr 2e-5 --max_datasets_size 50 --max_epochs 2
---OOM

root@83dccafe7db4:/workspace/yk_repo/ColossalAI/applications/Chat#
torchrun --standalone --nproc_per_node=3 /workspace/yk_repo/ColossalAI/applications/Chat/examples/train_reward_model.py --pretrain Coati-bloom-560m-sft --model 'bloom' --strategy colossalai_zero2 --loss_fn 'log_exp' --save_path Coati-bloom-560m-rw.pt --dataset 'Dahoas/rm-static'

torchrun --standalone --nproc_per_node=3 /workspace/yk_repo/ColossalAI/applications/Chat/examples/train_reward_model.py \
--pretrain Coati-bloom-560m-sft --model 'bloom' --strategy colossalai_zero2 --loss_fn 'log_exp' \
--save_path Coati-bloom-560m-rw.pt --dataset 'Anthropic/hh-rlhf'

数据集比较大，训练一次很长时间，可以通过
    parser.add_argument("--max_datasets_size", type=int, default=3000)
缩短训练数据！

root@83dccafe7db4:/workspace/yk_repo/ColossalAI/applications/Chat#
torchrun --standalone --nproc_per_node=3 /workspace/yk_repo/ColossalAI/applications/Chat/examples/train_prompts.py \
--prompt_dataset /share/hf_model/instinwild_ch.json     --pretrain_dataset /share/hf_model/instinwild_ch.json \
--strategy colossalai_zero2 --pretrain Coati-bloom-560m-sft --save_path Coati-bloom-560m-rl --model 'bloom' \
--rm_pretrain Coati-bloom-560m-sft --rm_path Coati-bloom-560m-rw.pt --max_datasets_size 500 --num_episodes 1


root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/palm#
bash run.sh  ==== passed




++++++++++++++++++++++++++
 # execute on the remote machine
fab_conn.run(cmds, hide=False)
错误：exception: not a valid RSA private key file
解决 # https://stackoverflow.com/questions/54612609/paramiko-not-a-valid-rsa-private-key-file
You should make a copy of id_rsa and convert it to RSA type with ssh-keygen.
To Convert "BEGIN OPENSSH PRIVATE KEY" to "BEGIN RSA PRIVATE KEY"
ssh-keygen -p -m PEM -f ~/.ssh/id_rsa
以上先基于C:\yk_repo\ds\DeepSpeedExamples\yknote-多机多卡.txt配置