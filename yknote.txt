https://github.com/SeekPoint/ColossalAI

amd00@MZ32-00:~/yk_repo/ColossalAI$
sudo docker pull hpcaitech/colossalai:0.3.0



d00@MZ32-00:~/yk_repo/ColossalAI$ sudo docker build -t colossalai ./docker


amd00@MZ32-00:~/yk_repo/ColossalAI$ sudo docker images
REPOSITORY                    TAG                            IMAGE ID       CREATED          SIZE
colossalai                    latest                         1f2f6a3e1dcf   15 minutes ago   17.2GB

# 浅析Docker容器的两种运行模式及 docker run 的 --rm 参数的作用及与 docker rm 的区别
sudo docker run --gpus=all --rm -it -v /home/amd00:/workspace -v /data:/share -w /workspace/yk_repo/ColossalAI --ipc=host --name colossal --shm-size="100G" colossalai:latest  #1f2f6a3e1dcf
sudo docker run --gpus=all -it -v /home/amd00:/workspace -v /data:/share -w /workspace/yk_repo/ColossalAI --ipc=host --name colossal --shm-size="100G" colossalai:latest  #1f2f6a3e1dcf

在Docker容器退出时，默认容器内部的文件系统仍然被保留，加上--rm就会被删除

所有pip可以用加速
-i https://pypi.tuna.tsinghua.edu.cn/simple
         pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
Collecting transformers>=4.23

====================================================================================================
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/opt# bash run_demo.sh
错误
ImportError: /opt/conda/lib/python3.9/site-packages/_cffi_backend.cpython-39-x86_64-linux-gnu.so: undefined symbol: ffi_type_uint32, version LIBFFI_BASE_7.0
解决
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/opt# pip install --upgrade cffi==1.14.0
Collecting cffi==1.14.0
目前卡在
RuntimeError: FlashAttention only supports Ampere GPUs or newer.

===================================================================================================

root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/gpt/hybridparallelism# bash run.sh
---修改若干参数后可以到达
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(
RuntimeError: FlashAttention only supports Ampere GPUs or newer.
----只能暂时强制屏蔽！


====================================================================================================

root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/gpt/gemini# bash run_gemini.sh
+ export DISTPLAN=CAI_Gemini
+ DISTPLAN=CAI_Gemini
+ export GPUNUM=1
====跑通！@
===================================================================================================
warnings.warn("please install xformers from https://github.com/facebookresearch/xformers")
必须修正
root@83dccafe7db4:/workspace/yk_repo/ColossalAI#
pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
或者源码安装
git clone https://github.com/facebookresearch/xformers.git
cd xformers
git submodule update --init --recursive
pip install -r requirements.txt
pip install -e .

===================================================================================================
错误
    # https://stackoverflow.com/questions/68084302/assertionerror-cannot-handle-batch-sizes-1-if-no-padding-token-is-defined
    # "AssertionError: Cannot handle batch sizes > 1 if no padding token is > defined" and pad_token = eos_token
修正    cfg.pad_token_id = cfg.eos_token_id

========================

module 'torch' has no attribute 'fx' 错误，
解决
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/llama2# pip install transformers==4.34.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
more tests:
4.35.0-4.35.2: same error
4.34.0-4.34.1: succeed


===================
跑通
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/llama2# colossalai run --nproc_per_node 3 pretrain.py
Epoch 0:   2%|▏         | 45/2423
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/examples/language/llama2# colossalai run --nproc_per_node 2 pretrain.py
Epoch 0:  12%|█▏        | 445/3634
可以看出，同样bs下，增加显卡数量可以减少迭代次数，


=========================
https://zhuanlan.zhihu.com/p/636605130
算法框架-LLM-4-ColossalAI源码分析(ing)
root@83dccafe7db4:/workspace/yk_repo/ColossalAI/applications/Chat# pip install -e .

root@83dccafe7db4:/workspace/yk_repo/ColossalAI/applications/Chat#
torchrun --standalone --nproc_per_node=3 /workspace/yk_repo/ColossalAI/applications/Chat/examples/train_sft.py     --pretrain "/share/hf_model/bloomz-560m/"     --model 'bloom'     --strategy colossalai_zero2     --save_path Coati-bloom-560m-sft     --dataset /share/hf_model/instinwild_ch.json     --batch_size 8     --accumulation_steps 8     --lr 2e-5     --max_datasets_size 512     --max_epochs 3
passed


